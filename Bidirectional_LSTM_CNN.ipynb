{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PtC1V18EusEm"
   },
   "outputs": [],
   "source": [
    "# Essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from collections import Counter\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, Bidirectional, LSTM\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kFL_cujcvNdm"
   },
   "outputs": [],
   "source": [
    "# Importing training and test data\n",
    "raw_data = pd.read_csv('train.csv', header=None, index_col=0)\n",
    "testing = pd.read_csv('test.csv',header=None, index_col=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JC4ywzcpvF-e"
   },
   "outputs": [],
   "source": [
    "# Splitting features and labels\n",
    "dataset = raw_data.values\n",
    "X = dataset[:, 0:960].astype(float)\n",
    "Y = dataset[:, 960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYSlza_ivSnl",
    "outputId": "997b54d5-cbaa-4654-a71a-4acf7d1600a3"
   },
   "outputs": [],
   "source": [
    "# Oversampling minority classes\n",
    "sm = SMOTE(k_neighbors=11, random_state=42)\n",
    "sm1 = SMOTETomek(smote=sm,random_state=42)\n",
    "\n",
    "X, Y = sm1.fit_resample(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-Ne1NCNTvZEC"
   },
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "scaler = scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ILG_pq74vaK2"
   },
   "outputs": [],
   "source": [
    "Y = Y.reshape(len(Y), 1)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "matrix_Y = enc.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "leBoC56ivdgz"
   },
   "outputs": [],
   "source": [
    "# Spliting training and validation data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, matrix_Y, test_size=0.3, random_state=42, stratify=matrix_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VuOUe6wOvgMo"
   },
   "outputs": [],
   "source": [
    "# Reshaping to use as input in CNN\n",
    "x_train = x_train.reshape(len(x_train), 16, 60, 1)\n",
    "x_test = x_test.reshape(len(x_test), 16, 60, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Um-9x4QEujk5",
    "outputId": "eda2ff2e-cf80-4c44-b55d-3ba7bc67791d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 16, 60, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16, 60, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 60, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 16, 60, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 60, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 8, 30, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 30, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 30, 64)         18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 30, 64)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 30, 64)         256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 30, 64)         36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 30, 64)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 30, 64)         256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 15, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 15, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 15, 128)        73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 15, 128)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 15, 128)        512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 15, 128)        147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 4, 15, 128)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 15, 128)        512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 2, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 2, 896)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 2, 256)            1049600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 192)               271104    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 49)                9457      \n",
      "=================================================================\n",
      "Total params: 1,618,385\n",
      "Trainable params: 1,617,489\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    '''\n",
    "    Returning learning rate according to the epoch number\n",
    "    '''\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    elif epoch > 125:\n",
    "        lrate = 0.0003       \n",
    "    return lrate\n",
    "\n",
    "\n",
    "def ReshapeLayer(x): \n",
    "    '''\n",
    "    Reshaping to 2D array to use as input\n",
    "    in Bi-LSTM layer\n",
    "    '''\n",
    "    shape = x.shape  \n",
    "    reshape = keras.layers.Reshape((shape[1], shape[2]*shape[3]))(x)\n",
    "    \n",
    "    return reshape\n",
    "\n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', \n",
    "                 bias_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay), \n",
    "                 input_shape=(16, 60, 1)\n",
    "                )\n",
    "         )\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', \n",
    "                 bias_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)\n",
    "                )\n",
    "         )\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', \n",
    "                 bias_initializer='glorot_uniform', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)\n",
    "                )\n",
    "         )\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', \n",
    "                 bias_initializer='glorot_uniform', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)\n",
    "                )\n",
    "         )\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', \n",
    "                 bias_initializer='glorot_uniform', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)\n",
    "                )\n",
    "         )\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', \n",
    "                 bias_initializer='glorot_uniform', \n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)\n",
    "                )\n",
    "         )\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(keras.layers.Lambda(ReshapeLayer))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(96)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(49, activation='softmax'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2LUMAn7v-AU",
    "outputId": "7fa6bee6-069c-4ccb-eb00-2c9983acd9b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "316/316 [==============================] - 88s 279ms/step - loss: 3.8985 - accuracy: 0.0328 - val_loss: 3.8145 - val_accuracy: 0.0610\n",
      "Epoch 2/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 3.7971 - accuracy: 0.0545 - val_loss: 3.6872 - val_accuracy: 0.0654\n",
      "Epoch 3/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 3.6826 - accuracy: 0.0627 - val_loss: 3.5747 - val_accuracy: 0.0810\n",
      "Epoch 4/200\n",
      "316/316 [==============================] - 85s 269ms/step - loss: 3.5780 - accuracy: 0.0785 - val_loss: 3.4772 - val_accuracy: 0.0975\n",
      "Epoch 5/200\n",
      "316/316 [==============================] - 80s 254ms/step - loss: 3.4932 - accuracy: 0.0945 - val_loss: 3.3915 - val_accuracy: 0.1415\n",
      "Epoch 6/200\n",
      "316/316 [==============================] - 80s 253ms/step - loss: 3.4149 - accuracy: 0.1173 - val_loss: 3.3032 - val_accuracy: 0.1659\n",
      "Epoch 7/200\n",
      "316/316 [==============================] - 80s 255ms/step - loss: 3.3395 - accuracy: 0.1359 - val_loss: 3.2224 - val_accuracy: 0.1943\n",
      "Epoch 8/200\n",
      "316/316 [==============================] - 80s 254ms/step - loss: 3.2668 - accuracy: 0.1578 - val_loss: 3.1402 - val_accuracy: 0.2138\n",
      "Epoch 9/200\n",
      "316/316 [==============================] - 79s 250ms/step - loss: 3.1936 - accuracy: 0.1737 - val_loss: 3.0547 - val_accuracy: 0.2290\n",
      "Epoch 10/200\n",
      "316/316 [==============================] - 80s 252ms/step - loss: 3.1195 - accuracy: 0.1971 - val_loss: 2.9702 - val_accuracy: 0.2576\n",
      "Epoch 11/200\n",
      "316/316 [==============================] - 79s 249ms/step - loss: 3.0425 - accuracy: 0.2168 - val_loss: 2.8751 - val_accuracy: 0.2812\n",
      "Epoch 12/200\n",
      "316/316 [==============================] - 79s 251ms/step - loss: 2.9652 - accuracy: 0.2327 - val_loss: 2.7994 - val_accuracy: 0.2942\n",
      "Epoch 13/200\n",
      "316/316 [==============================] - 79s 250ms/step - loss: 2.8888 - accuracy: 0.2534 - val_loss: 2.7156 - val_accuracy: 0.3165\n",
      "Epoch 14/200\n",
      "316/316 [==============================] - 79s 249ms/step - loss: 2.8086 - accuracy: 0.2758 - val_loss: 2.6468 - val_accuracy: 0.3314\n",
      "Epoch 15/200\n",
      "316/316 [==============================] - 79s 250ms/step - loss: 2.7334 - accuracy: 0.2899 - val_loss: 2.5319 - val_accuracy: 0.3529\n",
      "Epoch 16/200\n",
      "316/316 [==============================] - 80s 252ms/step - loss: 2.6520 - accuracy: 0.3117 - val_loss: 2.4356 - val_accuracy: 0.3791\n",
      "Epoch 17/200\n",
      "316/316 [==============================] - 79s 250ms/step - loss: 2.5737 - accuracy: 0.3274 - val_loss: 2.3604 - val_accuracy: 0.3926\n",
      "Epoch 18/200\n",
      "316/316 [==============================] - 79s 251ms/step - loss: 2.4932 - accuracy: 0.3510 - val_loss: 2.3016 - val_accuracy: 0.4044\n",
      "Epoch 19/200\n",
      "316/316 [==============================] - 81s 255ms/step - loss: 2.4162 - accuracy: 0.3703 - val_loss: 2.2220 - val_accuracy: 0.4262\n",
      "Epoch 20/200\n",
      "316/316 [==============================] - 79s 250ms/step - loss: 2.3462 - accuracy: 0.3809 - val_loss: 2.1031 - val_accuracy: 0.4480\n",
      "Epoch 21/200\n",
      "316/316 [==============================] - 79s 251ms/step - loss: 2.2776 - accuracy: 0.3994 - val_loss: 2.0462 - val_accuracy: 0.4577\n",
      "Epoch 22/200\n",
      "316/316 [==============================] - 80s 252ms/step - loss: 2.2141 - accuracy: 0.4125 - val_loss: 1.9860 - val_accuracy: 0.4693\n",
      "Epoch 23/200\n",
      "316/316 [==============================] - 80s 252ms/step - loss: 2.1497 - accuracy: 0.4312 - val_loss: 1.9233 - val_accuracy: 0.4854\n",
      "Epoch 24/200\n",
      "316/316 [==============================] - 80s 253ms/step - loss: 2.0938 - accuracy: 0.4453 - val_loss: 1.9019 - val_accuracy: 0.4859\n",
      "Epoch 25/200\n",
      "316/316 [==============================] - 80s 252ms/step - loss: 2.0281 - accuracy: 0.4573 - val_loss: 1.7791 - val_accuracy: 0.5225\n",
      "Epoch 26/200\n",
      "316/316 [==============================] - 79s 251ms/step - loss: 1.9751 - accuracy: 0.4710 - val_loss: 1.7505 - val_accuracy: 0.5258\n",
      "Epoch 27/200\n",
      "316/316 [==============================] - 80s 253ms/step - loss: 1.9214 - accuracy: 0.4867 - val_loss: 1.6884 - val_accuracy: 0.5383\n",
      "Epoch 28/200\n",
      "316/316 [==============================] - 80s 252ms/step - loss: 1.8680 - accuracy: 0.5010 - val_loss: 1.6039 - val_accuracy: 0.5634\n",
      "Epoch 29/200\n",
      "316/316 [==============================] - 85s 268ms/step - loss: 1.8091 - accuracy: 0.5160 - val_loss: 1.5653 - val_accuracy: 0.5800\n",
      "Epoch 30/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 1.7572 - accuracy: 0.5297 - val_loss: 1.5102 - val_accuracy: 0.5936\n",
      "Epoch 31/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 1.7018 - accuracy: 0.5461 - val_loss: 1.4638 - val_accuracy: 0.6023\n",
      "Epoch 32/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 1.6518 - accuracy: 0.5615 - val_loss: 1.4121 - val_accuracy: 0.6143\n",
      "Epoch 33/200\n",
      "316/316 [==============================] - 86s 271ms/step - loss: 1.6065 - accuracy: 0.5713 - val_loss: 1.3976 - val_accuracy: 0.6202\n",
      "Epoch 34/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 1.5659 - accuracy: 0.5779 - val_loss: 1.3050 - val_accuracy: 0.6455\n",
      "Epoch 35/200\n",
      "316/316 [==============================] - 88s 279ms/step - loss: 1.5181 - accuracy: 0.5918 - val_loss: 1.2769 - val_accuracy: 0.6550\n",
      "Epoch 36/200\n",
      "316/316 [==============================] - 100s 316ms/step - loss: 1.4716 - accuracy: 0.6032 - val_loss: 1.2173 - val_accuracy: 0.6685\n",
      "Epoch 37/200\n",
      "316/316 [==============================] - 96s 305ms/step - loss: 1.4392 - accuracy: 0.6124 - val_loss: 1.2159 - val_accuracy: 0.6730\n",
      "Epoch 38/200\n",
      "316/316 [==============================] - 82s 260ms/step - loss: 1.3928 - accuracy: 0.6297 - val_loss: 1.1790 - val_accuracy: 0.6780\n",
      "Epoch 39/200\n",
      "316/316 [==============================] - 66s 207ms/step - loss: 1.3552 - accuracy: 0.6341 - val_loss: 1.1294 - val_accuracy: 0.6918\n",
      "Epoch 40/200\n",
      "316/316 [==============================] - 68s 215ms/step - loss: 1.3154 - accuracy: 0.6433 - val_loss: 1.1058 - val_accuracy: 0.7010\n",
      "Epoch 41/200\n",
      "316/316 [==============================] - 68s 215ms/step - loss: 1.2778 - accuracy: 0.6564 - val_loss: 1.0508 - val_accuracy: 0.7133\n",
      "Epoch 42/200\n",
      "316/316 [==============================] - 69s 220ms/step - loss: 1.2592 - accuracy: 0.6618 - val_loss: 1.0455 - val_accuracy: 0.7164\n",
      "Epoch 43/200\n",
      "316/316 [==============================] - 72s 227ms/step - loss: 1.2074 - accuracy: 0.6753 - val_loss: 1.0230 - val_accuracy: 0.7236\n",
      "Epoch 44/200\n",
      "316/316 [==============================] - 74s 233ms/step - loss: 1.1847 - accuracy: 0.6832 - val_loss: 0.9989 - val_accuracy: 0.7281\n",
      "Epoch 45/200\n",
      "316/316 [==============================] - 77s 243ms/step - loss: 1.1578 - accuracy: 0.6891 - val_loss: 0.9626 - val_accuracy: 0.7431\n",
      "Epoch 46/200\n",
      "316/316 [==============================] - 86s 274ms/step - loss: 1.1285 - accuracy: 0.6946 - val_loss: 0.9523 - val_accuracy: 0.7422\n",
      "Epoch 47/200\n",
      "316/316 [==============================] - 83s 263ms/step - loss: 1.0988 - accuracy: 0.7007 - val_loss: 0.9149 - val_accuracy: 0.7509\n",
      "Epoch 48/200\n",
      "316/316 [==============================] - 81s 255ms/step - loss: 1.0637 - accuracy: 0.7130 - val_loss: 0.9083 - val_accuracy: 0.7556\n",
      "Epoch 49/200\n",
      "316/316 [==============================] - 82s 260ms/step - loss: 1.0481 - accuracy: 0.7173 - val_loss: 0.8587 - val_accuracy: 0.7713\n",
      "Epoch 50/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 1.0156 - accuracy: 0.7242 - val_loss: 0.8508 - val_accuracy: 0.7722\n",
      "Epoch 51/200\n",
      "316/316 [==============================] - 81s 255ms/step - loss: 1.0004 - accuracy: 0.7294 - val_loss: 0.8375 - val_accuracy: 0.7743\n",
      "Epoch 52/200\n",
      "316/316 [==============================] - 83s 264ms/step - loss: 0.9770 - accuracy: 0.7344 - val_loss: 0.8115 - val_accuracy: 0.7836\n",
      "Epoch 53/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 0.9537 - accuracy: 0.7420 - val_loss: 0.8033 - val_accuracy: 0.7871\n",
      "Epoch 54/200\n",
      "316/316 [==============================] - 80s 254ms/step - loss: 0.9294 - accuracy: 0.7480 - val_loss: 0.7898 - val_accuracy: 0.7913\n",
      "Epoch 55/200\n",
      "316/316 [==============================] - 85s 269ms/step - loss: 0.9087 - accuracy: 0.7529 - val_loss: 0.7794 - val_accuracy: 0.7911\n",
      "Epoch 56/200\n",
      "316/316 [==============================] - 82s 260ms/step - loss: 0.8924 - accuracy: 0.7606 - val_loss: 0.7822 - val_accuracy: 0.7938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "316/316 [==============================] - 82s 260ms/step - loss: 0.8685 - accuracy: 0.7666 - val_loss: 0.7479 - val_accuracy: 0.8031\n",
      "Epoch 58/200\n",
      "316/316 [==============================] - 83s 264ms/step - loss: 0.8551 - accuracy: 0.7698 - val_loss: 0.7246 - val_accuracy: 0.8102\n",
      "Epoch 59/200\n",
      "316/316 [==============================] - 82s 260ms/step - loss: 0.8318 - accuracy: 0.7746 - val_loss: 0.7246 - val_accuracy: 0.8084\n",
      "Epoch 60/200\n",
      "316/316 [==============================] - 83s 262ms/step - loss: 0.8181 - accuracy: 0.7775 - val_loss: 0.7125 - val_accuracy: 0.8122\n",
      "Epoch 61/200\n",
      "316/316 [==============================] - 83s 262ms/step - loss: 0.7968 - accuracy: 0.7853 - val_loss: 0.7055 - val_accuracy: 0.8132\n",
      "Epoch 62/200\n",
      "316/316 [==============================] - 81s 255ms/step - loss: 0.7742 - accuracy: 0.7925 - val_loss: 0.6769 - val_accuracy: 0.8229\n",
      "Epoch 63/200\n",
      "316/316 [==============================] - 83s 262ms/step - loss: 0.7666 - accuracy: 0.7900 - val_loss: 0.6809 - val_accuracy: 0.8211\n",
      "Epoch 64/200\n",
      "316/316 [==============================] - 83s 263ms/step - loss: 0.7535 - accuracy: 0.7964 - val_loss: 0.6503 - val_accuracy: 0.8274\n",
      "Epoch 65/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.7316 - accuracy: 0.8017 - val_loss: 0.6456 - val_accuracy: 0.8293\n",
      "Epoch 66/200\n",
      "316/316 [==============================] - 89s 281ms/step - loss: 0.7224 - accuracy: 0.8050 - val_loss: 0.6324 - val_accuracy: 0.8365\n",
      "Epoch 67/200\n",
      "316/316 [==============================] - 92s 290ms/step - loss: 0.7074 - accuracy: 0.8090 - val_loss: 0.6270 - val_accuracy: 0.8347\n",
      "Epoch 68/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.6999 - accuracy: 0.8135 - val_loss: 0.6334 - val_accuracy: 0.8325\n",
      "Epoch 69/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 0.6805 - accuracy: 0.8163 - val_loss: 0.6165 - val_accuracy: 0.8418\n",
      "Epoch 70/200\n",
      "316/316 [==============================] - 88s 280ms/step - loss: 0.6726 - accuracy: 0.8181 - val_loss: 0.6072 - val_accuracy: 0.8411\n",
      "Epoch 71/200\n",
      "316/316 [==============================] - 86s 272ms/step - loss: 0.6535 - accuracy: 0.8231 - val_loss: 0.6512 - val_accuracy: 0.8325\n",
      "Epoch 72/200\n",
      "316/316 [==============================] - 84s 265ms/step - loss: 0.6412 - accuracy: 0.8265 - val_loss: 0.6087 - val_accuracy: 0.8422\n",
      "Epoch 73/200\n",
      "316/316 [==============================] - 84s 265ms/step - loss: 0.6259 - accuracy: 0.8303 - val_loss: 0.6202 - val_accuracy: 0.8412\n",
      "Epoch 74/200\n",
      "316/316 [==============================] - 84s 266ms/step - loss: 0.6198 - accuracy: 0.8345 - val_loss: 0.5924 - val_accuracy: 0.8492\n",
      "Epoch 75/200\n",
      "316/316 [==============================] - 83s 262ms/step - loss: 0.6056 - accuracy: 0.8346 - val_loss: 0.6248 - val_accuracy: 0.8381\n",
      "Epoch 76/200\n",
      "316/316 [==============================] - 82s 261ms/step - loss: 0.5988 - accuracy: 0.8402 - val_loss: 0.5696 - val_accuracy: 0.8518\n",
      "Epoch 77/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 0.5711 - accuracy: 0.8463 - val_loss: 0.5779 - val_accuracy: 0.8516\n",
      "Epoch 78/200\n",
      "316/316 [==============================] - 82s 261ms/step - loss: 0.5656 - accuracy: 0.8504 - val_loss: 0.5554 - val_accuracy: 0.8563\n",
      "Epoch 79/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.5504 - accuracy: 0.8515 - val_loss: 0.5586 - val_accuracy: 0.8564\n",
      "Epoch 80/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.5438 - accuracy: 0.8554 - val_loss: 0.5486 - val_accuracy: 0.8601\n",
      "Epoch 81/200\n",
      "316/316 [==============================] - 84s 266ms/step - loss: 0.5390 - accuracy: 0.8559 - val_loss: 0.5508 - val_accuracy: 0.8575\n",
      "Epoch 82/200\n",
      "316/316 [==============================] - 86s 271ms/step - loss: 0.5430 - accuracy: 0.8537 - val_loss: 0.5650 - val_accuracy: 0.8576\n",
      "Epoch 83/200\n",
      "316/316 [==============================] - 81s 256ms/step - loss: 0.5258 - accuracy: 0.8614 - val_loss: 0.5454 - val_accuracy: 0.8597\n",
      "Epoch 84/200\n",
      "316/316 [==============================] - 81s 258ms/step - loss: 0.5253 - accuracy: 0.8561 - val_loss: 0.5511 - val_accuracy: 0.8629\n",
      "Epoch 85/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.5214 - accuracy: 0.8600 - val_loss: 0.5456 - val_accuracy: 0.8626\n",
      "Epoch 86/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.5191 - accuracy: 0.8615 - val_loss: 0.5504 - val_accuracy: 0.8631\n",
      "Epoch 87/200\n",
      "316/316 [==============================] - 81s 258ms/step - loss: 0.5072 - accuracy: 0.8630 - val_loss: 0.5411 - val_accuracy: 0.8629\n",
      "Epoch 88/200\n",
      "316/316 [==============================] - 81s 256ms/step - loss: 0.5051 - accuracy: 0.8661 - val_loss: 0.5293 - val_accuracy: 0.8657\n",
      "Epoch 89/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.5051 - accuracy: 0.8652 - val_loss: 0.5507 - val_accuracy: 0.8637\n",
      "Epoch 90/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.4889 - accuracy: 0.8703 - val_loss: 0.5478 - val_accuracy: 0.8626\n",
      "Epoch 91/200\n",
      "316/316 [==============================] - 81s 256ms/step - loss: 0.4977 - accuracy: 0.8683 - val_loss: 0.5369 - val_accuracy: 0.8642\n",
      "Epoch 92/200\n",
      "316/316 [==============================] - 84s 266ms/step - loss: 0.4801 - accuracy: 0.8716 - val_loss: 0.5437 - val_accuracy: 0.8638\n",
      "Epoch 93/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.4859 - accuracy: 0.8702 - val_loss: 0.5278 - val_accuracy: 0.8684\n",
      "Epoch 94/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.4784 - accuracy: 0.8713 - val_loss: 0.5382 - val_accuracy: 0.8644\n",
      "Epoch 95/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 0.4708 - accuracy: 0.8726 - val_loss: 0.5159 - val_accuracy: 0.8693\n",
      "Epoch 96/200\n",
      "316/316 [==============================] - 82s 261ms/step - loss: 0.4661 - accuracy: 0.8745 - val_loss: 0.5237 - val_accuracy: 0.8682\n",
      "Epoch 97/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.4602 - accuracy: 0.8752 - val_loss: 0.5259 - val_accuracy: 0.8713\n",
      "Epoch 98/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.4584 - accuracy: 0.8779 - val_loss: 0.5084 - val_accuracy: 0.8725\n",
      "Epoch 99/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.4560 - accuracy: 0.8824 - val_loss: 0.5322 - val_accuracy: 0.8679\n",
      "Epoch 100/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.4441 - accuracy: 0.8809 - val_loss: 0.5233 - val_accuracy: 0.8699\n",
      "Epoch 101/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.4468 - accuracy: 0.8818 - val_loss: 0.5106 - val_accuracy: 0.8734\n",
      "Epoch 102/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.4452 - accuracy: 0.8799 - val_loss: 0.5131 - val_accuracy: 0.8736\n",
      "Epoch 103/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.4474 - accuracy: 0.8798 - val_loss: 0.5166 - val_accuracy: 0.8721\n",
      "Epoch 104/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 0.4349 - accuracy: 0.8865 - val_loss: 0.5171 - val_accuracy: 0.8734\n",
      "Epoch 105/200\n",
      "316/316 [==============================] - 82s 260ms/step - loss: 0.4341 - accuracy: 0.8845 - val_loss: 0.5065 - val_accuracy: 0.8743\n",
      "Epoch 106/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 0.4278 - accuracy: 0.8863 - val_loss: 0.5166 - val_accuracy: 0.8725\n",
      "Epoch 107/200\n",
      "316/316 [==============================] - 83s 262ms/step - loss: 0.4267 - accuracy: 0.8868 - val_loss: 0.5135 - val_accuracy: 0.8733\n",
      "Epoch 108/200\n",
      "316/316 [==============================] - 82s 260ms/step - loss: 0.4338 - accuracy: 0.8850 - val_loss: 0.5092 - val_accuracy: 0.8741\n",
      "Epoch 109/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.4213 - accuracy: 0.8887 - val_loss: 0.5057 - val_accuracy: 0.8763\n",
      "Epoch 110/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.4139 - accuracy: 0.8902 - val_loss: 0.5101 - val_accuracy: 0.8745\n",
      "Epoch 111/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 0.4084 - accuracy: 0.8939 - val_loss: 0.5130 - val_accuracy: 0.8743\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 81s 257ms/step - loss: 0.4063 - accuracy: 0.8949 - val_loss: 0.5070 - val_accuracy: 0.8762\n",
      "Epoch 113/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.4038 - accuracy: 0.8935 - val_loss: 0.5045 - val_accuracy: 0.8754\n",
      "Epoch 114/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.3999 - accuracy: 0.8935 - val_loss: 0.4937 - val_accuracy: 0.8807\n",
      "Epoch 115/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.4050 - accuracy: 0.8911 - val_loss: 0.5024 - val_accuracy: 0.8791\n",
      "Epoch 116/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.3904 - accuracy: 0.8967 - val_loss: 0.4973 - val_accuracy: 0.8807\n",
      "Epoch 117/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.3919 - accuracy: 0.8966 - val_loss: 0.5016 - val_accuracy: 0.8787\n",
      "Epoch 118/200\n",
      "316/316 [==============================] - 82s 259ms/step - loss: 0.3891 - accuracy: 0.8963 - val_loss: 0.4946 - val_accuracy: 0.8823\n",
      "Epoch 119/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.3865 - accuracy: 0.8978 - val_loss: 0.5006 - val_accuracy: 0.8805\n",
      "Epoch 120/200\n",
      "316/316 [==============================] - 81s 256ms/step - loss: 0.3757 - accuracy: 0.9006 - val_loss: 0.4956 - val_accuracy: 0.8793\n",
      "Epoch 121/200\n",
      "316/316 [==============================] - 81s 258ms/step - loss: 0.3805 - accuracy: 0.8998 - val_loss: 0.4912 - val_accuracy: 0.8802\n",
      "Epoch 122/200\n",
      "316/316 [==============================] - 82s 258ms/step - loss: 0.3733 - accuracy: 0.9025 - val_loss: 0.5029 - val_accuracy: 0.8784\n",
      "Epoch 123/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.3723 - accuracy: 0.9002 - val_loss: 0.4990 - val_accuracy: 0.8802\n",
      "Epoch 124/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.3724 - accuracy: 0.9003 - val_loss: 0.4925 - val_accuracy: 0.8828\n",
      "Epoch 125/200\n",
      "316/316 [==============================] - 81s 257ms/step - loss: 0.3663 - accuracy: 0.9041 - val_loss: 0.4906 - val_accuracy: 0.8831\n",
      "Epoch 126/200\n",
      "316/316 [==============================] - 83s 261ms/step - loss: 0.3706 - accuracy: 0.9025 - val_loss: 0.4900 - val_accuracy: 0.8829\n",
      "Epoch 127/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 0.3622 - accuracy: 0.9059 - val_loss: 0.4999 - val_accuracy: 0.8795\n",
      "Epoch 128/200\n",
      "316/316 [==============================] - 83s 262ms/step - loss: 0.3586 - accuracy: 0.9068 - val_loss: 0.5001 - val_accuracy: 0.8812\n",
      "Epoch 129/200\n",
      "316/316 [==============================] - 87s 275ms/step - loss: 0.3565 - accuracy: 0.9062 - val_loss: 0.4928 - val_accuracy: 0.8810\n",
      "Epoch 130/200\n",
      "316/316 [==============================] - 87s 276ms/step - loss: 0.3549 - accuracy: 0.9070 - val_loss: 0.4982 - val_accuracy: 0.8824\n",
      "Epoch 131/200\n",
      "316/316 [==============================] - 85s 269ms/step - loss: 0.3480 - accuracy: 0.9079 - val_loss: 0.4871 - val_accuracy: 0.8842\n",
      "Epoch 132/200\n",
      "316/316 [==============================] - 87s 275ms/step - loss: 0.3521 - accuracy: 0.9092 - val_loss: 0.4937 - val_accuracy: 0.8823\n",
      "Epoch 133/200\n",
      "316/316 [==============================] - 87s 276ms/step - loss: 0.3449 - accuracy: 0.9087 - val_loss: 0.4797 - val_accuracy: 0.8863\n",
      "Epoch 134/200\n",
      "316/316 [==============================] - 85s 270ms/step - loss: 0.3336 - accuracy: 0.9123 - val_loss: 0.4891 - val_accuracy: 0.8832\n",
      "Epoch 135/200\n",
      "316/316 [==============================] - 84s 265ms/step - loss: 0.3421 - accuracy: 0.9095 - val_loss: 0.4848 - val_accuracy: 0.8859\n",
      "Epoch 136/200\n",
      "316/316 [==============================] - 85s 269ms/step - loss: 0.3365 - accuracy: 0.9118 - val_loss: 0.4842 - val_accuracy: 0.8847\n",
      "Epoch 137/200\n",
      "316/316 [==============================] - 85s 268ms/step - loss: 0.3317 - accuracy: 0.9149 - val_loss: 0.4873 - val_accuracy: 0.8843\n",
      "Epoch 138/200\n",
      "316/316 [==============================] - 85s 269ms/step - loss: 0.3332 - accuracy: 0.9125 - val_loss: 0.4851 - val_accuracy: 0.8850\n",
      "Epoch 139/200\n",
      "316/316 [==============================] - 86s 271ms/step - loss: 0.3260 - accuracy: 0.9142 - val_loss: 0.5015 - val_accuracy: 0.8828\n",
      "Epoch 140/200\n",
      "316/316 [==============================] - 86s 272ms/step - loss: 0.3225 - accuracy: 0.9160 - val_loss: 0.4926 - val_accuracy: 0.8861\n",
      "Epoch 141/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 0.3313 - accuracy: 0.9123 - val_loss: 0.4865 - val_accuracy: 0.8873\n",
      "Epoch 142/200\n",
      "316/316 [==============================] - 85s 269ms/step - loss: 0.3211 - accuracy: 0.9155 - val_loss: 0.4857 - val_accuracy: 0.8873\n",
      "Epoch 143/200\n",
      "316/316 [==============================] - 84s 266ms/step - loss: 0.3172 - accuracy: 0.9189 - val_loss: 0.4890 - val_accuracy: 0.8872\n",
      "Epoch 144/200\n",
      "316/316 [==============================] - 84s 266ms/step - loss: 0.3165 - accuracy: 0.9165 - val_loss: 0.4815 - val_accuracy: 0.8876\n",
      "Epoch 145/200\n",
      "316/316 [==============================] - 85s 269ms/step - loss: 0.3139 - accuracy: 0.9184 - val_loss: 0.4747 - val_accuracy: 0.8907\n",
      "Epoch 146/200\n",
      "316/316 [==============================] - 83s 264ms/step - loss: 0.3105 - accuracy: 0.9211 - val_loss: 0.4813 - val_accuracy: 0.8895\n",
      "Epoch 147/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 0.3093 - accuracy: 0.9193 - val_loss: 0.4951 - val_accuracy: 0.8846\n",
      "Epoch 148/200\n",
      "316/316 [==============================] - 88s 277ms/step - loss: 0.3042 - accuracy: 0.9215 - val_loss: 0.4843 - val_accuracy: 0.8882\n",
      "Epoch 149/200\n",
      "316/316 [==============================] - 89s 281ms/step - loss: 0.3019 - accuracy: 0.9221 - val_loss: 0.4868 - val_accuracy: 0.8878\n",
      "Epoch 150/200\n",
      "316/316 [==============================] - 90s 284ms/step - loss: 0.2986 - accuracy: 0.9219 - val_loss: 0.4806 - val_accuracy: 0.8890\n",
      "Epoch 151/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 0.3020 - accuracy: 0.9230 - val_loss: 0.4690 - val_accuracy: 0.8917\n",
      "Epoch 152/200\n",
      "316/316 [==============================] - 85s 268ms/step - loss: 0.3011 - accuracy: 0.9217 - val_loss: 0.4803 - val_accuracy: 0.8889\n",
      "Epoch 153/200\n",
      "316/316 [==============================] - 86s 271ms/step - loss: 0.2952 - accuracy: 0.9233 - val_loss: 0.4878 - val_accuracy: 0.8867\n",
      "Epoch 154/200\n",
      "316/316 [==============================] - 87s 275ms/step - loss: 0.2929 - accuracy: 0.9260 - val_loss: 0.4849 - val_accuracy: 0.8885\n",
      "Epoch 155/200\n",
      "316/316 [==============================] - 86s 274ms/step - loss: 0.2986 - accuracy: 0.9237 - val_loss: 0.4807 - val_accuracy: 0.8890\n",
      "Epoch 156/200\n",
      "316/316 [==============================] - 89s 280ms/step - loss: 0.2936 - accuracy: 0.9248 - val_loss: 0.4688 - val_accuracy: 0.8921\n",
      "Epoch 157/200\n",
      "316/316 [==============================] - 87s 274ms/step - loss: 0.2868 - accuracy: 0.9265 - val_loss: 0.4777 - val_accuracy: 0.8899\n",
      "Epoch 158/200\n",
      "316/316 [==============================] - 86s 272ms/step - loss: 0.2828 - accuracy: 0.9259 - val_loss: 0.4789 - val_accuracy: 0.8928\n",
      "Epoch 159/200\n",
      "316/316 [==============================] - 86s 272ms/step - loss: 0.2892 - accuracy: 0.9258 - val_loss: 0.4825 - val_accuracy: 0.8900\n",
      "Epoch 160/200\n",
      "316/316 [==============================] - 86s 272ms/step - loss: 0.2851 - accuracy: 0.9275 - val_loss: 0.4822 - val_accuracy: 0.8894\n",
      "Epoch 161/200\n",
      "316/316 [==============================] - 85s 268ms/step - loss: 0.2844 - accuracy: 0.9268 - val_loss: 0.4800 - val_accuracy: 0.8918\n",
      "Epoch 162/200\n",
      "316/316 [==============================] - 86s 271ms/step - loss: 0.2782 - accuracy: 0.9282 - val_loss: 0.4870 - val_accuracy: 0.8905\n",
      "Epoch 163/200\n",
      "316/316 [==============================] - 85s 268ms/step - loss: 0.2758 - accuracy: 0.9297 - val_loss: 0.4868 - val_accuracy: 0.8889\n",
      "Epoch 164/200\n",
      "316/316 [==============================] - 85s 268ms/step - loss: 0.2747 - accuracy: 0.9307 - val_loss: 0.4861 - val_accuracy: 0.8902\n",
      "Epoch 165/200\n",
      "316/316 [==============================] - 84s 266ms/step - loss: 0.2725 - accuracy: 0.9288 - val_loss: 0.4762 - val_accuracy: 0.8926\n",
      "Epoch 166/200\n",
      "316/316 [==============================] - 85s 268ms/step - loss: 0.2683 - accuracy: 0.9338 - val_loss: 0.4771 - val_accuracy: 0.8922\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316/316 [==============================] - 84s 265ms/step - loss: 0.2680 - accuracy: 0.9316 - val_loss: 0.4678 - val_accuracy: 0.8927\n",
      "Epoch 168/200\n",
      "316/316 [==============================] - 85s 270ms/step - loss: 0.2690 - accuracy: 0.9315 - val_loss: 0.4833 - val_accuracy: 0.8920\n",
      "Epoch 169/200\n",
      "316/316 [==============================] - 84s 267ms/step - loss: 0.2637 - accuracy: 0.9339 - val_loss: 0.4825 - val_accuracy: 0.8919\n",
      "Epoch 170/200\n",
      "316/316 [==============================] - 86s 274ms/step - loss: 0.2669 - accuracy: 0.9307 - val_loss: 0.4778 - val_accuracy: 0.8927\n",
      "Epoch 171/200\n",
      "316/316 [==============================] - 86s 272ms/step - loss: 0.2598 - accuracy: 0.9340 - val_loss: 0.4732 - val_accuracy: 0.8927\n",
      "Epoch 172/200\n",
      "316/316 [==============================] - 84s 264ms/step - loss: 0.2570 - accuracy: 0.9352 - val_loss: 0.4658 - val_accuracy: 0.8956\n",
      "Epoch 173/200\n",
      "316/316 [==============================] - 85s 270ms/step - loss: 0.2606 - accuracy: 0.9341 - val_loss: 0.4768 - val_accuracy: 0.8922\n",
      "Epoch 174/200\n",
      "316/316 [==============================] - 84s 265ms/step - loss: 0.2560 - accuracy: 0.9331 - val_loss: 0.4718 - val_accuracy: 0.8952\n",
      "Epoch 175/200\n",
      "316/316 [==============================] - 84s 266ms/step - loss: 0.2546 - accuracy: 0.9360 - val_loss: 0.4701 - val_accuracy: 0.8948\n",
      "Epoch 176/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 0.2524 - accuracy: 0.9365 - val_loss: 0.4812 - val_accuracy: 0.8929\n",
      "Epoch 177/200\n",
      "316/316 [==============================] - 86s 272ms/step - loss: 0.2582 - accuracy: 0.9345 - val_loss: 0.4761 - val_accuracy: 0.8958\n",
      "Epoch 178/200\n",
      "316/316 [==============================] - 85s 270ms/step - loss: 0.2539 - accuracy: 0.9343 - val_loss: 0.4754 - val_accuracy: 0.8942\n",
      "Epoch 179/200\n",
      "316/316 [==============================] - 86s 273ms/step - loss: 0.2485 - accuracy: 0.9382 - val_loss: 0.4719 - val_accuracy: 0.8947\n",
      "Epoch 180/200\n",
      "316/316 [==============================] - 89s 281ms/step - loss: 0.2402 - accuracy: 0.9406 - val_loss: 0.4797 - val_accuracy: 0.8939\n",
      "Epoch 181/200\n",
      "316/316 [==============================] - 85s 269ms/step - loss: 0.2475 - accuracy: 0.9397 - val_loss: 0.4747 - val_accuracy: 0.8949\n",
      "Epoch 182/200\n",
      "316/316 [==============================] - 86s 271ms/step - loss: 0.2458 - accuracy: 0.9394 - val_loss: 0.4711 - val_accuracy: 0.8942\n",
      "Epoch 183/200\n",
      "316/316 [==============================] - 85s 269ms/step - loss: 0.2443 - accuracy: 0.9375 - val_loss: 0.4683 - val_accuracy: 0.8960\n",
      "Epoch 184/200\n",
      "316/316 [==============================] - 84s 266ms/step - loss: 0.2318 - accuracy: 0.9435 - val_loss: 0.4793 - val_accuracy: 0.8934\n",
      "Epoch 185/200\n",
      "316/316 [==============================] - 88s 277ms/step - loss: 0.2345 - accuracy: 0.9407 - val_loss: 0.4773 - val_accuracy: 0.8951\n",
      "Epoch 186/200\n",
      "316/316 [==============================] - 85s 268ms/step - loss: 0.2359 - accuracy: 0.9432 - val_loss: 0.4875 - val_accuracy: 0.8914\n",
      "Epoch 187/200\n",
      "316/316 [==============================] - 87s 276ms/step - loss: 0.2414 - accuracy: 0.9400 - val_loss: 0.4706 - val_accuracy: 0.8962\n",
      "Epoch 188/200\n",
      "316/316 [==============================] - 89s 281ms/step - loss: 0.2368 - accuracy: 0.9410 - val_loss: 0.4687 - val_accuracy: 0.8948\n",
      "Epoch 189/200\n",
      "316/316 [==============================] - 93s 296ms/step - loss: 0.2336 - accuracy: 0.9423 - val_loss: 0.4790 - val_accuracy: 0.8943\n",
      "Epoch 190/200\n",
      "316/316 [==============================] - 96s 303ms/step - loss: 0.2283 - accuracy: 0.9438 - val_loss: 0.4787 - val_accuracy: 0.8944\n",
      "Epoch 191/200\n",
      "316/316 [==============================] - 97s 307ms/step - loss: 0.2332 - accuracy: 0.9422 - val_loss: 0.4740 - val_accuracy: 0.8949\n",
      "Epoch 192/200\n",
      "316/316 [==============================] - 97s 307ms/step - loss: 0.2227 - accuracy: 0.9460 - val_loss: 0.4786 - val_accuracy: 0.8945\n",
      "Epoch 193/200\n",
      "316/316 [==============================] - 93s 294ms/step - loss: 0.2254 - accuracy: 0.9452 - val_loss: 0.4751 - val_accuracy: 0.8951\n",
      "Epoch 194/200\n",
      "316/316 [==============================] - 96s 303ms/step - loss: 0.2297 - accuracy: 0.9425 - val_loss: 0.4720 - val_accuracy: 0.8971\n",
      "Epoch 195/200\n",
      "316/316 [==============================] - 103s 327ms/step - loss: 0.2303 - accuracy: 0.9432 - val_loss: 0.4727 - val_accuracy: 0.8974\n",
      "Epoch 196/200\n",
      "316/316 [==============================] - 105s 331ms/step - loss: 0.2272 - accuracy: 0.9456 - val_loss: 0.4762 - val_accuracy: 0.8964\n",
      "Epoch 197/200\n",
      "316/316 [==============================] - 107s 337ms/step - loss: 0.2236 - accuracy: 0.9446 - val_loss: 0.4638 - val_accuracy: 0.8982\n",
      "Epoch 198/200\n",
      "316/316 [==============================] - 105s 333ms/step - loss: 0.2190 - accuracy: 0.9468 - val_loss: 0.4795 - val_accuracy: 0.8948\n",
      "Epoch 199/200\n",
      "316/316 [==============================] - 104s 329ms/step - loss: 0.2208 - accuracy: 0.9472 - val_loss: 0.4667 - val_accuracy: 0.8963\n",
      "Epoch 200/200\n",
      "316/316 [==============================] - 126s 400ms/step - loss: 0.2201 - accuracy: 0.9459 - val_loss: 0.4767 - val_accuracy: 0.8950\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "batch_size = 64\n",
    " \n",
    "# Defining SGD optimizer\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                              mode=\"min\", patience=25, \n",
    "                                              restore_best_weights=True\n",
    "                                             )\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=200, \n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test,y_test),\n",
    "                    callbacks=[LearningRateScheduler(lr_schedule), earlystopping]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nakTZYFswF1E"
   },
   "outputs": [],
   "source": [
    "# Reshaping the test data\n",
    "sample_data = scaler.transform(testing)\n",
    "sample_data = sample_data.reshape(len(sample_data), 16, 60, 1)\n",
    "\n",
    "# Predicting the test data\n",
    "y_pred = model.predict(sample_data)\n",
    "\n",
    "# Converting one hot encoded values back to categorical labels\n",
    "y_pred = enc.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4P9aw4ZDwQ8O"
   },
   "outputs": [],
   "source": [
    "# Reading the sample csv file to replicate a file like that for test prediction data\n",
    "sample = pd.read_csv('sample.csv')\n",
    "submission = pd.DataFrame(columns=sample.columns)\n",
    "submission['Id']=sample['Id']\n",
    "submission['Category']=pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6HOO6DWXwTsf"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"final_model.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Bidirectional LSTM CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
